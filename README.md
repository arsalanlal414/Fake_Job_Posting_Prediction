# Fake Job Postings Prediction Project

## ğŸ“‹ Project Overview
This project aims to detect fraudulent job postings using machine learning techniques. With the rise of online job platforms, identifying fake job postings has become crucial to protect job seekers from scams.

## ğŸ¯ Objective
Build a binary classification model to predict whether a job posting is fraudulent (1) or legitimate (0).

## ğŸ“Š Dataset
- **Training Data**: 9,999 job postings
- **Test Data**: 7,882 job postings
- **Features**: 18+ columns including text and categorical data

### Key Features:
- **Text Features**: title, location, company_profile, description, requirements, benefits
- **Categorical Features**: employment_type, required_experience, required_education, industry, function
- **Binary Features**: telecommuting, has_company_logo, has_questions
- **Numerical Features**: salary_range (needs parsing)
- **Target**: fraudulent (0 = legitimate, 1 = fake)

## ğŸ“ Project Structure
```
fake_job_prediction_project/
â”œâ”€â”€ data/                          # Dataset files
â”‚   â”œâ”€â”€ fake_job_postings_train.csv
â”‚   â”œâ”€â”€ fake_job_postings_test.csv
â”‚   â””â”€â”€ submit_example.csv
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_exploratory_data_analysis.ipynb
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚   â””â”€â”€ 04_model_training.ipynb
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ models/                       # Saved models
â”œâ”€â”€ results/                      # Model outputs and predictions
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸ› ï¸ Technologies
- Python 3.8+
- Pandas, NumPy
- Scikit-learn
- Matplotlib, Seaborn
- NLTK/spaCy for text processing
- XGBoost/LightGBM (optional)

## ğŸš€ Getting Started

### Installation
```bash
pip install -r requirements.txt
```

### Usage
1. **Data Exploration**: Start with `notebooks/01_exploratory_data_analysis.ipynb`
2. **Preprocessing**: Clean and prepare data
3. **Feature Engineering**: Create meaningful features
4. **Model Training**: Train and evaluate models
5. **Prediction**: Generate predictions for test data

## ğŸ“ˆ Methodology
1. **Data Analysis**: Understand patterns in fraudulent vs legitimate postings
2. **Data Cleaning**: Handle missing values, outliers
3. **Feature Engineering**: 
   - Text features (TF-IDF, word counts, sentiment)
   - Categorical encoding
   - New features (has_salary, description_length, etc.)
4. **Model Selection**: Try multiple algorithms
   - Logistic Regression (baseline)
   - Random Forest
   - XGBoost
   - Neural Networks (advanced)
5. **Evaluation**: Precision, Recall, F1-Score, AUC-ROC
6. **Handle Imbalance**: SMOTE, class weights, or ensemble methods

## ğŸ“Š Expected Deliverables
- [ ] Comprehensive EDA report
- [ ] Cleaned and processed dataset
- [ ] Trained classification models
- [ ] Model comparison analysis
- [ ] Final predictions on test set
- [ ] Documentation and presentation

## Contributers
Muhammad Arsalan
Qazi Naveed Ur Rehman
Mannan Aleem
Prashant Lamichhane

## ğŸ“… Timeline
- Week 1: Data Exploration & Preprocessing
- Week 2: Feature Engineering
- Week 3: Model Training & Evaluation
- Week 4: Final Testing & Documentation

## ğŸ“ Notes
- Fraudulent job postings are typically rare (imbalanced dataset)
- Focus on Precision and Recall balance
- Text features are likely very important
- Consider ensemble methods for better performance
